<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>Euro-Truck-Simulator-2-Lane-Assist.plugins.UFLDLaneDetection.UFLD.ultrafastLaneDetector.exportLib.ultrafastLaneV2.layer API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/ir-black.min.css" crossorigin>
<style>
:root {
--highlight-color: #202;
}
html {
scrollbar-face-color: #646464;
scrollbar-base-color: #646464;
scrollbar-3dlight-color: #646464;
scrollbar-highlight-color: #646464;
scrollbar-track-color: #000;
scrollbar-arrow-color: #000;
scrollbar-shadow-color: #646464;
scrollbar-dark-shadow-color: #646464;
}
::-webkit-scrollbar { width: 14px; height: 3px;}
::-webkit-scrollbar-button {
background-color: #000;
height: 3px;}
::-webkit-scrollbar-track {
background-color: #646464;}
::-webkit-scrollbar-track-piece { background-color: #000;}
::-webkit-scrollbar-thumb { height: 50px; background-color: #666; border-radius: 7px;}
::-webkit-scrollbar-corner { background-color: #646464;}
::-webkit-resizer { background-color: #666;}
.flex {
display: flex !important;
}
body {
line-height: 1.5em;
color: #fff;
background-color: #1e1e1e;
font: 14px/1.5 Helvetica, Arial, sans-serif;
margin: 0;
padding: 0;
}
#content {
padding: 20px;
}
#sidebar {
padding: 30px;
overflow: hidden;
}
#sidebar>*:last-child {
margin-bottom: 2cm;
}
.http-server-breadcrumbs {
font-size: 130%;
margin: 0 0 15px 0;
}
#footer {
font-size: .75em;
padding: 5px 30px;
border-top: 1px solid #fff;
text-align: right;
}
#footer p {
margin: 0 0 0 1em;
display: inline-block;
}
#footer p:last-child {
margin-right: 30px;
}
h1,
h2,
h3,
h4,
h5 {
font-weight: 300;
color: #fff;
}
h1 {
font-size: 2.5em;
line-height: 1.1em;
}
h2 {
font-size: 1.75em;
margin: 1em 0 .50em 0;
}
h3 {
font-size: 1.4em;
margin: 25px 0 10px 0;
}
h4 {
margin: 0;
font-size: 105%;
}
h1:target,
h2:target,
h3:target,
h4:target,
h5:target,
h6:target {
background: #1e1e1e;
padding: 0.2em 0;
}
a {
color: #8fd6fc;
text-decoration: none;
transition: color .3s ease-in-out;
}
a:hover {
color: #00a4fc;
}
.title code {
font-weight: bold;
}
h2[id^="header-"] {
margin-top: 2em;
}
.ident {
color: #f6fc85;
}
strong {
color: #8fd6fc;
}
pre code {
background: transparent;
font-size: .8em;
line-height: 1.4em;
}
code {
background: rgba(255, 255, 255, 0.1);
padding: 1px 4px;
overflow-wrap: break-word;
}
h1 code {
background: transparent
}
pre {
background: transparent;
border: 0;
border-top: 1px solid #ccc;
border-bottom: 1px solid #ccc;
margin: 1em 0;
padding: 1ex;
}
#http-server-module-list {
display: flex;
flex-flow: column;
}
#http-server-module-list div {
display: flex;
}
#http-server-module-list dt {
min-width: 10%;
}
#http-server-module-list p {
margin-top: 0;
}
.toc ul,
#index {
list-style-type: none;
margin: 0;
padding: 0;
}
#index code {
background: transparent;
}
#index h3 {
border-bottom: 1px solid #ddd;
}
#index ul {
padding: 0;
}
#index h4 {
margin-top: .6em;
font-weight: bold;
}
/* Make TOC lists have 2+ columns when viewport is wide enough.
Assuming ~20-character identifiers and ~30% wide sidebar. */
@media (min-width: 200ex) {
#index .two-column {
column-count: 2
}
}
@media (min-width: 300ex) {
#index .two-column {
column-count: 3
}
}
dl {
margin-bottom: 2em;
}
dl dl:last-child {
margin-bottom: 4em;
}
dd {
margin: 0 0 1em 3em;
}
#header-classes+dl>dd {
margin-bottom: 3em;
}
dd dd {
margin-left: 2em;
}
dd p {
margin: 10px 0;
}
.name {
background: #000;
font-weight: bold;
font-size: .85em;
padding: 5px 10px;
display: inline-block;
min-width: 40%;
}
.name:hover {
background: #0b0b0b;
}
dt:target .name {
background: var(--highlight-color);
}
.name>span:first-child {
white-space: nowrap;
}
.name.class>span:nth-child(2) {
margin-left: .4em;
}
.inherited {
color: #999;
border-left: 5px solid #eee;
padding-left: 1em;
}
.inheritance em {
font-style: normal;
font-weight: bold;
}
.pydefname {
color: #f90;
}
/* Docstrings titles, e.g. in numpydoc format */
.desc h2 {
font-weight: 400;
font-size: 1.25em;
}
.desc h3 {
font-size: 1em;
}
.desc dt code {
background: inherit;
/* Don't grey-back parameters */
}
.source summary,
.git-link-div {
color: #666;
text-align: right;
font-weight: 400;
font-size: .8em;
text-transform: uppercase;
}
.source summary>* {
white-space: nowrap;
cursor: pointer;
}
.git-link {
color: inherit;
margin-left: 1em;
}
.source pre {
max-height: 500px;
overflow: auto;
margin: 0;
}
.source pre code {
font-size: 12px;
overflow: visible;
}
.hlist {
list-style: none;
}
.hlist li {
display: inline;
}
.hlist li:after {
content: ',\2002';
}
.hlist li:last-child:after {
content: none;
}
.hlist .hlist {
display: inline;
padding-left: 1em;
}
img {
max-width: 100%;
}
td {
padding: 0 .5em;
}
.admonition {
padding: .1em .5em;
margin-bottom: 1em;
}
.admonition-title {
font-weight: bold;
}
.admonition.note,
.admonition.info,
.admonition.important {
background: #00a;
}
.admonition.todo,
.admonition.versionadded,
.admonition.tip,
.admonition.hint {
background: #050;
}
.admonition.warning,
.admonition.versionchanged,
.admonition.deprecated {
background: #ec0;
}
.admonition.error,
.admonition.danger,
.admonition.caution {
background: #c20010;
}
p {
color: #fff;
}
</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>Euro-Truck-Simulator-2-Lane-Assist.plugins.UFLDLaneDetection.UFLD.ultrafastLaneDetector.exportLib.ultrafastLaneV2.layer</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import torch
from torch import nn

def initialize_weights(*models):
    for model in models:
        real_init_weights(model)
def real_init_weights(m):

    if isinstance(m, list):
        for mini_m in m:
            real_init_weights(mini_m)
    else:
        if isinstance(m, torch.nn.Conv2d):    
            torch.nn.init.kaiming_normal_(m.weight, nonlinearity=&#39;relu&#39;)
            if m.bias is not None:
                torch.nn.init.constant_(m.bias, 0)
        elif isinstance(m, torch.nn.Linear):
            m.weight.data.normal_(0.0, std=0.01)
        elif isinstance(m, torch.nn.BatchNorm2d):
            torch.nn.init.constant_(m.weight, 1)
            torch.nn.init.constant_(m.bias, 0)
        elif isinstance(m,torch.nn.Module):
            for mini_m in m.children():
                real_init_weights(mini_m)
        else:
            print(&#39;unkonwn module&#39;, m)

class AddCoordinates(object):

    r&#34;&#34;&#34;Coordinate Adder Module as defined in &#39;An Intriguing Failing of
    Convolutional Neural Networks and the CoordConv Solution&#39;
    (https://arxiv.org/pdf/1807.03247.pdf).
    This module concatenates coordinate information (`x`, `y`, and `r`) with
    given input tensor.
    `x` and `y` coordinates are scaled to `[-1, 1]` range where origin is the
    center. `r` is the Euclidean distance from the center and is scaled to
    `[0, 1]`.
    Args:
        with_r (bool, optional): If `True`, adds radius (`r`) coordinate
            information to input image. Default: `False`
    Shape:
        - Input: `(N, C_{in}, H_{in}, W_{in})`
        - Output: `(N, (C_{in} + 2) or (C_{in} + 3), H_{in}, W_{in})`
    Examples:
        &gt;&gt;&gt; coord_adder = AddCoordinates(True)
        &gt;&gt;&gt; input = torch.randn(8, 3, 64, 64)
        &gt;&gt;&gt; output = coord_adder(input)
        &gt;&gt;&gt; coord_adder = AddCoordinates(True)
        &gt;&gt;&gt; input = torch.randn(8, 3, 64, 64).cuda()
        &gt;&gt;&gt; output = coord_adder(input)
        &gt;&gt;&gt; device = torch.device(&#34;cuda:0&#34;)
        &gt;&gt;&gt; coord_adder = AddCoordinates(True)
        &gt;&gt;&gt; input = torch.randn(8, 3, 64, 64).to(device)
        &gt;&gt;&gt; output = coord_adder(input)
    &#34;&#34;&#34;

    def __init__(self, with_r=False):
        self.with_r = with_r

    def __call__(self, image):
        batch_size, _, image_height, image_width = image.size()

        y_coords = 2.0 * torch.arange(image_height).unsqueeze(
            1).expand(image_height, image_width) / (image_height - 1.0) - 1.0
        x_coords = 2.0 * torch.arange(image_width).unsqueeze(
            0).expand(image_height, image_width) / (image_width - 1.0) - 1.0

        coords = torch.stack((y_coords, x_coords), dim=0)

        if self.with_r:
            rs = ((y_coords ** 2) + (x_coords ** 2)) ** 0.5
            rs = rs / torch.max(rs)
            rs = torch.unsqueeze(rs, dim=0)
            coords = torch.cat((coords, rs), dim=0)

        coords = torch.unsqueeze(coords, dim=0).repeat(batch_size, 1, 1, 1)

        image = torch.cat((coords.to(image.device), image), dim=1)

        return image


class CoordConv(nn.Module):

    r&#34;&#34;&#34;2D Convolution Module Using Extra Coordinate Information as defined
    in &#39;An Intriguing Failing of Convolutional Neural Networks and the
    CoordConv Solution&#39; (https://arxiv.org/pdf/1807.03247.pdf).
    Args:
        Same as `torch.nn.Conv2d` with two additional arguments
        with_r (bool, optional): If `True`, adds radius (`r`) coordinate
            information to input image. Default: `False`
    Shape:
        - Input: `(N, C_{in}, H_{in}, W_{in})`
        - Output: `(N, C_{out}, H_{out}, W_{out})`
    Examples:
        &gt;&gt;&gt; coord_conv = CoordConv(3, 16, 3, with_r=True)
        &gt;&gt;&gt; input = torch.randn(8, 3, 64, 64)
        &gt;&gt;&gt; output = coord_conv(input)
        &gt;&gt;&gt; coord_conv = CoordConv(3, 16, 3, with_r=True).cuda()
        &gt;&gt;&gt; input = torch.randn(8, 3, 64, 64).cuda()
        &gt;&gt;&gt; output = coord_conv(input)
        &gt;&gt;&gt; device = torch.device(&#34;cuda:0&#34;)
        &gt;&gt;&gt; coord_conv = CoordConv(3, 16, 3, with_r=True).to(device)
        &gt;&gt;&gt; input = torch.randn(8, 3, 64, 64).to(device)
        &gt;&gt;&gt; output = coord_conv(input)
    &#34;&#34;&#34;

    def __init__(self, in_channels, out_channels, kernel_size,
                 stride=1, padding=0, dilation=1, groups=1, bias=True,
                 with_r=False):
        super(CoordConv, self).__init__()

        in_channels += 2
        if with_r:
            in_channels += 1

        self.conv_layer = nn.Conv2d(in_channels, out_channels,
                                    kernel_size, stride=stride,
                                    padding=padding, dilation=dilation,
                                    groups=groups, bias=bias)

        self.coord_adder = AddCoordinates(with_r)

    def forward(self, x):
        x = self.coord_adder(x)
        x = self.conv_layer(x)

        return x</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="Euro-Truck-Simulator-2-Lane-Assist.plugins.UFLDLaneDetection.UFLD.ultrafastLaneDetector.exportLib.ultrafastLaneV2.layer.initialize_weights"><code class="name flex">
<span>def <span class="ident">initialize_weights</span></span>(<span>*models)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize_weights(*models):
    for model in models:
        real_init_weights(model)</code></pre>
</details>
</dd>
<dt id="Euro-Truck-Simulator-2-Lane-Assist.plugins.UFLDLaneDetection.UFLD.ultrafastLaneDetector.exportLib.ultrafastLaneV2.layer.real_init_weights"><code class="name flex">
<span>def <span class="ident">real_init_weights</span></span>(<span>m)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def real_init_weights(m):

    if isinstance(m, list):
        for mini_m in m:
            real_init_weights(mini_m)
    else:
        if isinstance(m, torch.nn.Conv2d):    
            torch.nn.init.kaiming_normal_(m.weight, nonlinearity=&#39;relu&#39;)
            if m.bias is not None:
                torch.nn.init.constant_(m.bias, 0)
        elif isinstance(m, torch.nn.Linear):
            m.weight.data.normal_(0.0, std=0.01)
        elif isinstance(m, torch.nn.BatchNorm2d):
            torch.nn.init.constant_(m.weight, 1)
            torch.nn.init.constant_(m.bias, 0)
        elif isinstance(m,torch.nn.Module):
            for mini_m in m.children():
                real_init_weights(mini_m)
        else:
            print(&#39;unkonwn module&#39;, m)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="Euro-Truck-Simulator-2-Lane-Assist.plugins.UFLDLaneDetection.UFLD.ultrafastLaneDetector.exportLib.ultrafastLaneV2.layer.AddCoordinates"><code class="flex name class">
<span>class <span class="ident">AddCoordinates</span></span>
<span>(</span><span>with_r=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Coordinate Adder Module as defined in 'An Intriguing Failing of
Convolutional Neural Networks and the CoordConv Solution'
(<a href="https://arxiv.org/pdf/1807.03247.pdf">https://arxiv.org/pdf/1807.03247.pdf</a>).
This module concatenates coordinate information (<code>x</code>, <code>y</code>, and <code>r</code>) with
given input tensor.
<code>x</code> and <code>y</code> coordinates are scaled to <code>[-1, 1]</code> range where origin is the
center. <code>r</code> is the Euclidean distance from the center and is scaled to
<code>[0, 1]</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>with_r</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If <code>True</code>, adds radius (<code>r</code>) coordinate
information to input image. Default: <code>False</code></dd>
</dl>
<h2 id="shape">Shape</h2>
<ul>
<li>Input: <code>(N, C_{in}, H_{in}, W_{in})</code></li>
<li>Output: <code>(N, (C_{in} + 2) or (C_{in} + 3), H_{in}, W_{in})</code></li>
</ul>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; coord_adder = AddCoordinates(True)
&gt;&gt;&gt; input = torch.randn(8, 3, 64, 64)
&gt;&gt;&gt; output = coord_adder(input)
&gt;&gt;&gt; coord_adder = AddCoordinates(True)
&gt;&gt;&gt; input = torch.randn(8, 3, 64, 64).cuda()
&gt;&gt;&gt; output = coord_adder(input)
&gt;&gt;&gt; device = torch.device(&quot;cuda:0&quot;)
&gt;&gt;&gt; coord_adder = AddCoordinates(True)
&gt;&gt;&gt; input = torch.randn(8, 3, 64, 64).to(device)
&gt;&gt;&gt; output = coord_adder(input)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AddCoordinates(object):

    r&#34;&#34;&#34;Coordinate Adder Module as defined in &#39;An Intriguing Failing of
    Convolutional Neural Networks and the CoordConv Solution&#39;
    (https://arxiv.org/pdf/1807.03247.pdf).
    This module concatenates coordinate information (`x`, `y`, and `r`) with
    given input tensor.
    `x` and `y` coordinates are scaled to `[-1, 1]` range where origin is the
    center. `r` is the Euclidean distance from the center and is scaled to
    `[0, 1]`.
    Args:
        with_r (bool, optional): If `True`, adds radius (`r`) coordinate
            information to input image. Default: `False`
    Shape:
        - Input: `(N, C_{in}, H_{in}, W_{in})`
        - Output: `(N, (C_{in} + 2) or (C_{in} + 3), H_{in}, W_{in})`
    Examples:
        &gt;&gt;&gt; coord_adder = AddCoordinates(True)
        &gt;&gt;&gt; input = torch.randn(8, 3, 64, 64)
        &gt;&gt;&gt; output = coord_adder(input)
        &gt;&gt;&gt; coord_adder = AddCoordinates(True)
        &gt;&gt;&gt; input = torch.randn(8, 3, 64, 64).cuda()
        &gt;&gt;&gt; output = coord_adder(input)
        &gt;&gt;&gt; device = torch.device(&#34;cuda:0&#34;)
        &gt;&gt;&gt; coord_adder = AddCoordinates(True)
        &gt;&gt;&gt; input = torch.randn(8, 3, 64, 64).to(device)
        &gt;&gt;&gt; output = coord_adder(input)
    &#34;&#34;&#34;

    def __init__(self, with_r=False):
        self.with_r = with_r

    def __call__(self, image):
        batch_size, _, image_height, image_width = image.size()

        y_coords = 2.0 * torch.arange(image_height).unsqueeze(
            1).expand(image_height, image_width) / (image_height - 1.0) - 1.0
        x_coords = 2.0 * torch.arange(image_width).unsqueeze(
            0).expand(image_height, image_width) / (image_width - 1.0) - 1.0

        coords = torch.stack((y_coords, x_coords), dim=0)

        if self.with_r:
            rs = ((y_coords ** 2) + (x_coords ** 2)) ** 0.5
            rs = rs / torch.max(rs)
            rs = torch.unsqueeze(rs, dim=0)
            coords = torch.cat((coords, rs), dim=0)

        coords = torch.unsqueeze(coords, dim=0).repeat(batch_size, 1, 1, 1)

        image = torch.cat((coords.to(image.device), image), dim=1)

        return image</code></pre>
</details>
</dd>
<dt id="Euro-Truck-Simulator-2-Lane-Assist.plugins.UFLDLaneDetection.UFLD.ultrafastLaneDetector.exportLib.ultrafastLaneV2.layer.CoordConv"><code class="flex name class">
<span>class <span class="ident">CoordConv</span></span>
<span>(</span><span>in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, with_r=False)</span>
</code></dt>
<dd>
<div class="desc"><p>2D Convolution Module Using Extra Coordinate Information as defined
in 'An Intriguing Failing of Convolutional Neural Networks and the
CoordConv Solution' (<a href="https://arxiv.org/pdf/1807.03247.pdf">https://arxiv.org/pdf/1807.03247.pdf</a>).</p>
<h2 id="args">Args</h2>
<dl>
<dt>Same as <code>torch.nn.Conv2d</code> with two additional arguments</dt>
<dt><strong><code>with_r</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If <code>True</code>, adds radius (<code>r</code>) coordinate
information to input image. Default: <code>False</code></dd>
</dl>
<h2 id="shape">Shape</h2>
<ul>
<li>Input: <code>(N, C_{in}, H_{in}, W_{in})</code></li>
<li>Output: <code>(N, C_{out}, H_{out}, W_{out})</code></li>
</ul>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; coord_conv = CoordConv(3, 16, 3, with_r=True)
&gt;&gt;&gt; input = torch.randn(8, 3, 64, 64)
&gt;&gt;&gt; output = coord_conv(input)
&gt;&gt;&gt; coord_conv = CoordConv(3, 16, 3, with_r=True).cuda()
&gt;&gt;&gt; input = torch.randn(8, 3, 64, 64).cuda()
&gt;&gt;&gt; output = coord_conv(input)
&gt;&gt;&gt; device = torch.device(&quot;cuda:0&quot;)
&gt;&gt;&gt; coord_conv = CoordConv(3, 16, 3, with_r=True).to(device)
&gt;&gt;&gt; input = torch.randn(8, 3, 64, 64).to(device)
&gt;&gt;&gt; output = coord_conv(input)
</code></pre>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CoordConv(nn.Module):

    r&#34;&#34;&#34;2D Convolution Module Using Extra Coordinate Information as defined
    in &#39;An Intriguing Failing of Convolutional Neural Networks and the
    CoordConv Solution&#39; (https://arxiv.org/pdf/1807.03247.pdf).
    Args:
        Same as `torch.nn.Conv2d` with two additional arguments
        with_r (bool, optional): If `True`, adds radius (`r`) coordinate
            information to input image. Default: `False`
    Shape:
        - Input: `(N, C_{in}, H_{in}, W_{in})`
        - Output: `(N, C_{out}, H_{out}, W_{out})`
    Examples:
        &gt;&gt;&gt; coord_conv = CoordConv(3, 16, 3, with_r=True)
        &gt;&gt;&gt; input = torch.randn(8, 3, 64, 64)
        &gt;&gt;&gt; output = coord_conv(input)
        &gt;&gt;&gt; coord_conv = CoordConv(3, 16, 3, with_r=True).cuda()
        &gt;&gt;&gt; input = torch.randn(8, 3, 64, 64).cuda()
        &gt;&gt;&gt; output = coord_conv(input)
        &gt;&gt;&gt; device = torch.device(&#34;cuda:0&#34;)
        &gt;&gt;&gt; coord_conv = CoordConv(3, 16, 3, with_r=True).to(device)
        &gt;&gt;&gt; input = torch.randn(8, 3, 64, 64).to(device)
        &gt;&gt;&gt; output = coord_conv(input)
    &#34;&#34;&#34;

    def __init__(self, in_channels, out_channels, kernel_size,
                 stride=1, padding=0, dilation=1, groups=1, bias=True,
                 with_r=False):
        super(CoordConv, self).__init__()

        in_channels += 2
        if with_r:
            in_channels += 1

        self.conv_layer = nn.Conv2d(in_channels, out_channels,
                                    kernel_size, stride=stride,
                                    padding=padding, dilation=dilation,
                                    groups=groups, bias=bias)

        self.coord_adder = AddCoordinates(with_r)

    def forward(self, x):
        x = self.coord_adder(x)
        x = self.conv_layer(x)

        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Euro-Truck-Simulator-2-Lane-Assist.plugins.UFLDLaneDetection.UFLD.ultrafastLaneDetector.exportLib.ultrafastLaneV2.layer.CoordConv.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    x = self.coord_adder(x)
    x = self.conv_layer(x)

    return x</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="Euro-Truck-Simulator-2-Lane-Assist.plugins.UFLDLaneDetection.UFLD.ultrafastLaneDetector.exportLib.ultrafastLaneV2" href="index.html">Euro-Truck-Simulator-2-Lane-Assist.plugins.UFLDLaneDetection.UFLD.ultrafastLaneDetector.exportLib.ultrafastLaneV2</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="Euro-Truck-Simulator-2-Lane-Assist.plugins.UFLDLaneDetection.UFLD.ultrafastLaneDetector.exportLib.ultrafastLaneV2.layer.initialize_weights" href="#Euro-Truck-Simulator-2-Lane-Assist.plugins.UFLDLaneDetection.UFLD.ultrafastLaneDetector.exportLib.ultrafastLaneV2.layer.initialize_weights">initialize_weights</a></code></li>
<li><code><a title="Euro-Truck-Simulator-2-Lane-Assist.plugins.UFLDLaneDetection.UFLD.ultrafastLaneDetector.exportLib.ultrafastLaneV2.layer.real_init_weights" href="#Euro-Truck-Simulator-2-Lane-Assist.plugins.UFLDLaneDetection.UFLD.ultrafastLaneDetector.exportLib.ultrafastLaneV2.layer.real_init_weights">real_init_weights</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="Euro-Truck-Simulator-2-Lane-Assist.plugins.UFLDLaneDetection.UFLD.ultrafastLaneDetector.exportLib.ultrafastLaneV2.layer.AddCoordinates" href="#Euro-Truck-Simulator-2-Lane-Assist.plugins.UFLDLaneDetection.UFLD.ultrafastLaneDetector.exportLib.ultrafastLaneV2.layer.AddCoordinates">AddCoordinates</a></code></h4>
</li>
<li>
<h4><code><a title="Euro-Truck-Simulator-2-Lane-Assist.plugins.UFLDLaneDetection.UFLD.ultrafastLaneDetector.exportLib.ultrafastLaneV2.layer.CoordConv" href="#Euro-Truck-Simulator-2-Lane-Assist.plugins.UFLDLaneDetection.UFLD.ultrafastLaneDetector.exportLib.ultrafastLaneV2.layer.CoordConv">CoordConv</a></code></h4>
<ul class="">
<li><code><a title="Euro-Truck-Simulator-2-Lane-Assist.plugins.UFLDLaneDetection.UFLD.ultrafastLaneDetector.exportLib.ultrafastLaneV2.layer.CoordConv.forward" href="#Euro-Truck-Simulator-2-Lane-Assist.plugins.UFLDLaneDetection.UFLD.ultrafastLaneDetector.exportLib.ultrafastLaneV2.layer.CoordConv.forward">forward</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>